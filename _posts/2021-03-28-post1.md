---
title: 'AuthNet'
date: 2021-03-28
permalink: /posts/2021/03/post1/
tags:
  - ML
  - CV
---

<p align = "justify"> Authentication systems have been around for quite some time now. Simple passwords, patterns, codes, fingerprints, iris, audio recognition, and finally face recognition are some of the prevalent biometric systems available today for authentication. Facial recognition has even become very popular in smartphones in the past few years, and it has been used in many other security systems previously.</p>

![alt text](https://github.com/Pravanop/pravanop.github.io/blob/master/summary.jpg?raw=true)

## Why do we need to do better?
---
<p align = "justify"> Facial recognition has issues. Sophisticated systems have depth analysis in their algorithms, but the common face recognition systems can be fooled with a picture or by wearing glasses even. Fingerprints, passwords, patterns have their kryptonites. Fingerprints can be extracted from a sleeping person, passwords and patterns can be determined. Hence, we decided to make an enhanced biometric system, trying to better upon facial recognition. The important link between all these was that you could hack into these systems without the person being physically present in front of the device. To overcome this issue, we thought we would revisit a concept that was not touched upon in the past decade: <i> Lip password </i>. </p> 

## What is a Lip Password?
---
A user can choose their password, then they can stand in front of a camera and say that password. The combination of the password and the user’s face will be used to determine the authenticity of the user. What does this solve? If the user never utters their password, except in front of the camera, nobody can record it and hence use it to invade your privacy. <br>
Surprisingly, with these advantages and added security for a lip password, they are relatively unvisited. There were two major reasons for this:
* Lip-reading has not been a  very accurate task till recent times (1), Even state of the art models have the best accuracy at 95%. If one used this work to recognize lip passwords then it would mean that for every 1000 unlocks, 50 would be misclassified. This is a dangerous amount of misclassifications for a biometric system.
* Further, to improve accuracy (near 100), lip passwords have to be restricted to some words, letters or digits. This would result in smaller sampler space resulting in lesser combinations and hence easier to learn. To limit a lip password would again make it weaker, and defeat the purpose of our efforts.

These two reasons severely limited the extent of research in lip passwords. These were also the important limitations that we kept in the back of our head as we designed AuthNet. <br>
As we explain the model that we built, we will also elucidate how we solved these problems and more. 

## Breaking down AuthNet
---
Let us dissect the manner in which AuthNet must approach authentication. There are two tasks to be addressed here: <br>
1) Extracting facial features of the user, if the facial features are used to recognize the person, the task is known as Face recognition. <br>
2) ‘Reading the lip’ i.e capture the movement of the lip as the password is uttered. This is called a lip-reading task. This is in essence the time-dependent movement of a lip. <br>

Task 1 has been solved (by solved I mean obtained 100% accuracy) by the state of the art convolutional neural networks like (add models), while Task 2 is the reason for the birth of Recurrent Neural Networks. <br>

Hence we decided to use both CNNs and RNNs together to perform our function. However, before that let us exactly define what our task is.

### The problem at hand:
The definition of AuthNet’s problem to solve is a major differentiation factor from other approaches. We decided to <i> not learn </i> what the person is saying, just whether they are saying the right password. This might seem confusing, let me break it down. 

<p align = "justify"> We have two facets to a lip password, the password and the user uttering it. Let us join these two facets into a single phrase - ‘person-word combination’. For the device to unlock, the right person has to be saying the right word. Out of all the possible passwords and of all the people on this planet, only one person-password combination is the right match. Therefore, we can define this as a binary classification task with the right person-word combination as 1 and everything else as 0. This simplifies the multilabel multiclass classification problem that was lip-reading to a binary classification problem that AuthNet is solving. The greatest advantage of this approach is that it is language invariant, one does not need to use dictionary English words, and doesn’t even have to use words at all. As long as it’s pronounceable in a reliable manner, it works! </p> <br>
<i> Mind you, this approach only works for this application. Conventional lip reading approaches are vital for tasks like subtitle generation in sans audio videos. </i><br>

That is the problem definition, now let us look at the structure of AuthNet. <br>

## How we built it
--
To make it simpler to understand, let’s look at one whole pass of this authentication system. The process has two important distinctions; <b>Setup</b> and <b>Usage</b>.

### Setup:
<p align = "justify"> Our user, Jane Doe, will be asked to speak her password 5 times in front of a camera. This will be a positive person-word combination. Then she will be asked to speak two other random words/ characters/ incantations of her choice. These two will act as the same person but with different word combinations. Authnet has a pre-existing database of different people saying different words that will also be used in negative samples. </p> <br>
<p align = "justify"> Having covered the same person and different person cases, we then oversample (aka duplicate) the small sample of positive cases we have (just 5 utterances) and then pass them into our ML model. </p><br>
<p align = "justify"> Jane Doe provides us videos, we splice them at a predefined frame rate to give us 10-15 sequential images of Jane uttering her words, as shown in Figure 1. Then we pad this sequence with white images to give us a fixed length (20 is what we went with). We pass each image to a CNN and then concatenate all the resulting ‘feature vectors’ we obtain. Thereafter we pass it into our LSTM model, which trains on these feature vectors and predicts 0/1. </p><br>

<p align = "justify"> To leverage the use of amazing CNN models trained for days on the huge datasets we decided to use the pre-trained VGGFace model, which is quite robust in classifying faces. The VGGNet base architecture [2] is a huge model (add image) and took up nearly 4GB of memory with the weights. This is a slight disadvantage to AuthNet. However, we don’t need to train the model, and this helps in a lesser set up times. Moving on, we removed the softmax layer of the model and extracted a 2622 vector that represents the facial features of a person, as shown in Figure 2. This feature vector is concatenated with the other and is fed to the LSTM, as shown in Figure 3. The actual training occurs in the LSTM section only and it is quite fast. We then download our model for the next process, Usage. Setup is a one time process.</p><br>

### Usage:
Now usage follows almost the same flow as Setup, except that we use the downloaded model to make predictions and allow/block access to the device. This step is super quick, but not quite the speed of face recognitions in OnePlus and iPhones.<br>

## Wrapping Things up
---
<p align = "justify"> This is AuthNet, a deep learning based authentication mechanism using temporal facial feature movements and we believe it improves upon certain key shortcomings in face recognition systems. To understand the nitty and gritty details of training, experimentation and testing check out our paper (3). You can check out our code and run the model (4).</p>
<p align = "justify"> In conclusion, AuthNet is a lip password based authentication mechanism, that allows users flexibility in language and vocabulary. There is a need for improvements made in memory and speed to be able to match face and fingerprint recognition, but we believe we made a step in the right direction towards better and safer devices. </p>

---

### References:
(1) LipNet: End-to-End Sentence-level Lipreading, https://arxiv.org/abs/1611.01599 <br>
(2) Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556 <br>
(3) AuthNet: A Deep Learning Based Authentication Mechanism Using Temporal Facial Feature Movements, https://arxiv.org/abs/2012.02515 <br>
(4) https://github.com/Mohit-Mithra/AuthNet <Br>











